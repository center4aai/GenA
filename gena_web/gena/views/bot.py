import streamlit as st
import os
import requests
import random
import pandas as pd
import tempfile
from PIL import Image
import base64
from datetime import datetime

from gena.config import API_CHANKS_URL, CHUNKS_DIR, API_GEN_QUE_URL, API_DATASET_URL, DOCS_DIR, LOGO
from gena.http import get, post, put, delete

def get_base64_image(img_path):
    with open(img_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode()

if CHUNKS_DIR:
    os.makedirs(CHUNKS_DIR, exist_ok=True)

st.title("GENA 2.0: Generation & sensitivity Assessment Russian language Framework")

if os.path.exists(LOGO):
    img_base64 = get_base64_image(LOGO)
    st.markdown(
        f"""
        <div style="display: flex; justify-content: center;">
            <img src="data:image/png;base64,{img_base64}" width="520"/>
        </div>
        """,
        unsafe_allow_html=True,
    )

st.markdown("---")
st.markdown("""
## üêä About GenA:
GenA (Generation & sensitivity Assessment) is a dynamic Russian-language framework designed to address the gap between classical question generation (QG) and practical requirements for legally accurate, socially safe questions, providing users with a controllable and transparent tool while minimizing conflict risks associated with sensitive topics (including those arising from cultural, historical, and linguistic considerations).

The framework provides three key contributions: first, generating diverse questions from Russian texts (exemplified with socially significant texts from trusted sources such as encyclopedias, regulatory legal acts, and legal documents); second, automatically assigning each question sensitivity levels that reflects conflict potential within sociocultural contexts, and third, incorporating human-in-the-loop validation processes. Our dual-validation methodology for sensitivity assessment combined with human intervention allows users to evaluate question quality, accept or reject generated questions, and validate or modify assigned sensitivity levels, thereby fostering continuous improvement and ensuring methodological rigor.

The generation involved 3 question types:
- single-choice, where one correct answer is selected from multiple options;
- multiple-choice, where several correct answers are selected from the options provided;
- open-ended with no options provided.

Consequently, each question receives specialized sensitivity annotation ranging from 1 to 3, based on the perceived probability of conflict emergence within a given sociocultural context at a fixed point in time and reflects the sensitivity level that a particular topic presents to respondents: 
- Low-sensitivity questions (Level 1) pertain to neutral topics that do not foster discussion or differences of opinion;
- Medium-sensitivity questions (Level 2) may address controversial or ambiguous topics. While different viewpoints exist, they are not radically opposed. Discussions may lead to lively debates but do not result in serious conflicts;
- High-sensitivity questions (Level 3) address highly sensitive cultural, historical, or political topics. Responses to such questions may require expressing personal opinions on contentious issues.

Users can evaluate both question generation quality and sensitivity level appropriateness, forming a crucial feedback loop for continuous dynamic service improvement.
            """)
st.markdown("---")

st.markdown("""

## üìò How to use GenA:

Follow these steps to generate questions from a document:

1. **Upload a document**  
   Click the "Upload a document" button and select a `.docx`, `.txt`, or `.pdf` file from your computer.

2. **Wait for processing**  
   The document will be split into logical chunks. You‚Äôll see a confirmation message once it‚Äôs done.

3. **Select question types**  
   Choose one or more question types from the following options:
   - `one` ‚Äì Single correct answer
   - `multi` ‚Äì Multiple correct answers
   - `open` ‚Äì Open-ended question

   > If no types are selected, all of them will be used by default.

4. **Generate questions**  
   Click the **Generate Questions** button to start generating tasks for all chunks in the document. For each chunk, one question of each selected type will be generated.

5. **View the results**  
   For each chunk, you'll see:
   - The **source text** (click to expand)
   - The **task**
   - A numbered list of **options**
   - The **Correct Answer**
   - The **Provocativeness Score**
   - The **Validation Score** (quality assessment)
   - Detailed validation breakdown (click to expand)

> After viewing the results, please click the **Confirm** button to validate the sensitivity and provocativeness level for each question.


6. **Download as CSV**  
   Once done, click the ‚Äúüì• Download as CSV‚Äù button to export all results as a spreadsheet.
""")


st.markdown("---")
st.markdown("### üìÑ Document template file for example")
example_path = os.path.join(DOCS_DIR, "Family_code_Russian_Federation_1-4.docx")
if os.path.exists(example_path):
    with open(example_path, "rb") as f:
        st.download_button(
            label="üì• Download document template file",
            data=f,
            file_name="Family_code_Russian_Federation_1-4.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            key="template_download"
        )
else:
    st.warning("–§–∞–π–ª —à–∞–±–ª–æ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω.")

st.markdown(" ")
st.markdown("---")
st.markdown("### ‚¨ÜÔ∏è Upload your document for start generation")


#–õ–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
uploaded_file = st.file_uploader("Upload a document", type=['docx', 'txt', 'pdf'])

if uploaded_file is not None:
    st.success(f"File uploaded: {uploaded_file.name}")

    dataset_name = st.text_input("Dataset Name:", value=f"Dataset_{uploaded_file.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    dataset_description = st.text_area("Dataset Description (optional):", value=f"Generated from {uploaded_file.name}")
    st.markdown("---")

    with tempfile.NamedTemporaryFile(delete=False, suffix=uploaded_file.name.split('.')[-1]) as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    if not API_CHANKS_URL:
        st.error("‚õî API_CHANKS_URL is not configured. Please check your environment variables.")
        st.stop()
        
    with open(tmp_path, "rb") as f:
        files = {"file": (uploaded_file.name, f, uploaded_file.type)}
        try:
            response = requests.post(API_CHANKS_URL, files=files)
            
        except requests.exceptions.ConnectionError:
            st.error("‚õî Could not connect to the FastAPI server. Make sure it is running.")
            st.stop()

    os.remove(tmp_path)
    if response.status_code == 200:
        data = response.json()
        st.success(f"‚úÖ Document successfully splited into chunks. Number of chunks: {data['num_chunks']}")
    else:
        st.error(f"Server error: {response.status_code}")
        st.json(response.text)
    
    question_types = st.multiselect(
        "Select question types to generate:",
        ['one', 'multi', 'open']
    )
    
    if not question_types:
        question_types = ['one', 'multi', 'open']

    # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã
    processing_mode = st.radio(
        "Processing Mode:",
        ["Queue Mode (Recommended)", "Direct Processing"],
        help="Queue Mode: Add tasks to queue for background processing. Direct Processing: Process immediately (may be slower)."
    )
    
    if st.button("Generate Questions"):
        chunks = data.get("chunks", [])
        total_chunks = len(chunks)
        
        if total_chunks == 0:
            st.error("No chunks found in the document.")
            st.stop()
        
        total_questions = len(chunks) * len(question_types)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º results –¥–ª—è –æ–±–æ–∏—Ö —Ä–µ–∂–∏–º–æ–≤
        results = []
        
        if processing_mode == "Queue Mode (Recommended)":
            st.info(f"üìä Adding {total_chunks} chunks √ó {len(question_types)} question types = {total_questions} total tasks to queue...")
            
            # –°–æ–∑–¥–∞–µ–º –æ—á–µ—Ä–µ–¥—å –¥–ª—è —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            queue_name = f"queue_{dataset_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            try:
                # –°–æ–∑–¥–∞–µ–º –æ—á–µ—Ä–µ–¥—å
                queue_payload = {
                    "name": queue_name,
                    "description": f"Queue for {dataset_name}",
                    "priority": 1
                }
                
                queue_response = post("/queues/", json=queue_payload)
                
                if queue_response.status_code != 200:
                    st.error(f"Failed to create queue: {queue_response.status_code}")
                    st.stop()
                
                # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å—Ä–∞–∑—É –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—á–µ—Ä–µ–¥–∏
                dataset_payload = {
                    "name": dataset_name,
                    "description": dataset_description,
                    "source_document": uploaded_file.name,
                    "questions": [],  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ - –æ–Ω–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è –ø–æ –º–µ—Ä–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    "metadata": {
                        "queue_name": queue_name,
                        "question_types": question_types,
                        "num_chunks_processed": 0,
                        "total_chunks": len(chunks),
                        "total_questions_generated": 0,
                        "questions_per_chunk": len(question_types),
                        "created_at": datetime.now().isoformat(),
                        "status": "processing"
                    }
                }
                
                dataset_response = post("/datasets/", json=dataset_payload)
                
                if dataset_response.status_code != 200:
                    st.error(f"Failed to create dataset: {dataset_response.status_code}")
                    st.stop()
                
                dataset_result = dataset_response.json()
                st.success(f"‚úÖ Created dataset '{dataset_name}' with ID: {dataset_result['dataset_id']}")
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –æ—á–µ—Ä–µ–¥—å
                tasks = []
                for idx, chunk in enumerate(chunks, 1):
                    for question_type in question_types:
                        task_data = {
                            "chunk_id": idx,
                            "chunk_text": chunk,
                            "question_type": question_type,
                            "source_document": uploaded_file.name,
                            "dataset_name": dataset_name,
                            "dataset_id": dataset_result['dataset_id'],  # –î–æ–±–∞–≤–ª—è–µ–º ID –¥–∞—Ç–∞—Å–µ—Ç–∞
                            "dataset_description": dataset_description,
                            "priority": 1
                        }
                        tasks.append(task_data)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å
                tasks_response = post(f"/queues/{queue_name}/tasks/", json=tasks)
                
                if tasks_response.status_code == 200:
                    result = tasks_response.json()
                    st.success(f"‚úÖ Successfully added {result['tasks_added']} tasks to queue '{queue_name}'")
                    st.info(f"üìã Queue: {queue_name}")
                    st.info(f"üìä Dataset: {dataset_name} (ID: {dataset_result['dataset_id']})")
                    st.info(f"üîß Tasks will be processed by the worker in the background")
                    st.info(f"üìä You can monitor progress in the Queue Manager page")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
                    st.markdown(f"""
                    ### üìã Monitor Progress
                    Go to the **Queue Manager** page to monitor the progress of your tasks.
                    Queue name: `{queue_name}`
                    Dataset name: `{dataset_name}`
                    """)
                else:
                    st.error(f"Failed to add tasks to queue: {tasks_response.status_code}")
                    
            except Exception as e:
                st.error(f"Error creating queue: {str(e)}")
        
        else:  # Direct Processing
            st.info(f"üìä Processing {total_chunks} chunks √ó {len(question_types)} question types = {total_questions} total questions...")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            if not API_GEN_QUE_URL:
                st.error("‚õî API_GEN_QUE_URL is not configured. Please check your environment variables.")
                st.stop()
                
            generate_url = API_GEN_QUE_URL

            question_counter = 0
            total_questions = len(chunks) * len(question_types)
            
            for idx, chunk in enumerate(chunks, 1):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
                for question_type in question_types:
                    question_counter += 1
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                    progress = question_counter / total_questions
                    progress_bar.progress(progress)
                    status_text.text(f"Processing chunk {idx}/{total_chunks}, question type: {question_type} ({question_counter}/{total_questions})...")

                    payload = {
                        "prompt": chunk,
                        "question_type": question_type,
                        "source": "user_input",
                        "chat_id": 12345
                    }

                    try:
                        res = requests.post(generate_url, json=payload)
                        if res.status_code == 200:
                            output = res.json().get("result", {}).get("output", {})
                            gq = output.get("generated_question", {})
                            sensitivity = output.get("sensitivity_score", {})
                            validation = output.get("validation_result", {})

                            # –í—Å–µ –Ω–µ–ø—É—Å—Ç—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞ option_*
                            options = [
                                (int(k.replace("option_", "")), v)
                                for k, v in gq.items()
                                if k.startswith("option_") and v not in [None, "None"]
                            ]
                            options.sort()
                            options_text = "\n".join(f"{i}. {text}" for i, text in options) if options else "No options provided"
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º options –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
                            options_dict = {}
                            for k, v in gq.items():
                                if k.startswith("option_") and v not in [None, "None"]:
                                    options_dict[k] = v

                            results.append({
                                "Chunk #": idx,
                                "Source Chunk": chunk,
                                "Question Type": output.get("question_type", "unknown"),
                                "Task": gq.get("task", ""),
                                "Options": options_text,
                                "Options Dict": options_dict,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ë–î
                                "Correct Answer": gq.get("outputs", "N/A"),
                                "Provocativeness": sensitivity.get("provocativeness_score", "N/A"),
                                "Validation Score": f"{validation.get('total', 'N/A')}/{validation.get('max_total', 'N/A')}",
                                "Validation Threshold": validation.get('threshold', 'N/A'),
                                "Validation Passed": validation.get('passed', False),
                                "Validation Details": validation.get('by_block', {})
                            })
                        else:
                            results.append({
                                "Chunk #": idx,
                                "Source Chunk": chunk,
                                "Question Type": question_type,
                                "Task": f"Error: {res.status_code}",
                                "Options": "",
                                "Correct Answer": "",
                                "Provocativeness": "",
                                "Validation Score": "N/A",
                                "Validation Threshold": "N/A",
                                "Validation Passed": False,
                                "Validation Details": {}
                            })
                    except requests.exceptions.RequestException as e:
                        results.append({
                            "Chunk #": idx,
                            "Source Chunk": chunk,
                            "Question Type": question_type,
                            "Task": f"Network error: {str(e)}",
                            "Options": "",
                            "Correct Answer": "",
                            "Provocativeness": "",
                            "Validation Score": "N/A",
                            "Validation Threshold": "N/A",
                            "Validation Passed": False,
                            "Validation Details": {}
                        })

            # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
            progress_bar.progress(1.0)
            status_text.text("‚úÖ Generation completed!")
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–æ–∫
            df = pd.DataFrame(results)
            st.success("‚úÖ Generation completed.")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å (–¥–ª—è Direct Processing)
        if results:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —á–∞–Ω–∫–∞–º –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            chunks_with_questions = {}
            for item in results:
                chunk_id = item['Chunk #']
                if chunk_id not in chunks_with_questions:
                    chunks_with_questions[chunk_id] = []
                chunks_with_questions[chunk_id].append(item)
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —á–∞–Ω–∫–∞–º
            for chunk_id, questions in chunks_with_questions.items():
                st.markdown("---")
                st.markdown(f"### üß© Chunk #{chunk_id}")
            
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è —á–∞–Ω–∫–∞
                if questions and questions[0].get('Source Chunk'):
                    with st.expander("üìÑ View Source Text", expanded=False):
                        st.markdown("**Source Text:**")
                        st.markdown(f"```\n{questions[0]['Source Chunk']}\n```")
                
                # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞
                for i, item in enumerate(questions, 1):
                    st.markdown(f"#### Question {i} ({item['Question Type']})")
                    st.markdown(f"**Task:** {item['Task']}")
                    st.markdown("**Options:**")
                    st.markdown(item['Options'])
                    st.markdown(f"**Correct Answer:** {item['Correct Answer']}")
                    
                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–∫–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"**Provocativeness Score:** {item['Provocativeness']}")
                    
                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    with col2:
                        validation_score = item['Validation Score']
                        validation_threshold = item.get('Validation Threshold', 'N/A')
                        validation_passed = item['Validation Passed']
                        
                        if validation_passed:
                            st.markdown(f"**Validation:** ‚úÖ {validation_score} (Threshold: {validation_threshold}) PASSED")
                        else:
                            st.markdown(f"**Validation:** ‚ùå {validation_score} (Threshold: {validation_threshold}) FAILED")
                    
                    # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    if item['Validation Details']:
                        with st.expander("üîç View Validation Details", expanded=False):
                            validation_details = item['Validation Details']
                            st.markdown("**Validation Breakdown:**")
                            
                            for block_name, scores in validation_details.items():
                                if isinstance(scores, list):
                                    total_score = sum(scores)
                                    st.markdown(f"- **{block_name}**: {scores} (Total: {total_score})")
                                else:
                                    st.markdown(f"- **{block_name}**: {scores}")
                    
                    if i < len(questions):
                        st.markdown("---")

        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ CSV —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–æ–∫
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ MongoDB —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–¥–ª—è Direct Processing)
        if results:
            st.markdown("---")
            st.markdown("### üíæ Save Dataset to Database")
            
            try:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                questions_data = []
                for item in results:
                    questions_data.append({
                        "chunk_id": item['Chunk #'],
                        "source_chunk": item['Source Chunk'],
                        "question_type": item['Question Type'],
                        "task": item['Task'],
                        "options": item.get('Options Dict', item['Options']),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å –µ—Å–ª–∏ –µ—Å—Ç—å
                        "correct_answer": str(item['Correct Answer']),
                        "provocativeness": str(item['Provocativeness']),
                        "validation_score": str(item['Validation Score']),
                        "validation_threshold": str(item.get('Validation Threshold', 'N/A')),
                        "validation_passed": str(item['Validation Passed']),
                        "validation_details": str(item['Validation Details'])
                    })
                
                # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
                dataset_payload = {
                    "name": dataset_name,
                    "description": dataset_description,
                    "source_document": uploaded_file.name,
                    "questions": questions_data,
                    "metadata": {
                        "question_types": question_types,
                        "num_chunks_processed": len(chunks),
                        "total_chunks": len(chunks),
                        "total_questions_generated": len(results),
                        "questions_per_chunk": len(question_types),
                        "generated_at": datetime.now().isoformat()
                    }
                }
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                st.write("Debug: Dataset payload:", dataset_payload)
                save_response = post("/datasets/", json=dataset_payload)
                
                if save_response.status_code == 200:
                    save_result = save_response.json()
                    st.success(f"‚úÖ Dataset saved successfully! Dataset ID: {save_result['dataset_id']}")
                    st.info(f"üìä Dataset '{dataset_name}' (version {save_result['version']}) has been saved to the database.")
                else:
                    st.error(f"‚ùå Error saving dataset: {save_response.status_code}")
                    st.error(f"Response text: {save_response.text}")
                    st.json(save_response.text)
                    
                csv = df.to_csv(index=False).encode('utf-8')
                st.download_button("üì• Download as CSV", data=csv, file_name="generated_tasks.csv", mime="text/csv", key="results_csv_download")
                    
            except requests.exceptions.ConnectionError:
                st.error("‚õî Could not connect to the Dataset API server. Make sure it is running.")
            except Exception as e:
                st.error(f"‚ùå Error saving dataset: {str(e)}")
